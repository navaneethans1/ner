{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyvw\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vowpal Wabbit class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequenceLabeler(pyvw.SearchTask):\n",
    "    def __init__(self, vw, sch, num_actions):\n",
    "        # you must must must initialize the parent class\n",
    "        # this will automatically store self.sch <- sch, self.vw <- vw\n",
    "        pyvw.SearchTask.__init__(self, vw, sch, num_actions)\n",
    "        \n",
    "        # set whatever options you want\n",
    "        sch.set_options( sch.AUTO_HAMMING_LOSS | sch.AUTO_CONDITION_FEATURES )\n",
    "\n",
    "    def _run(self, sentence):   # it's called _run to remind you that you shouldn't call it directly!\n",
    "        output = []\n",
    "        for n in range(len(sentence)):\n",
    "            pos,word = sentence[n]\n",
    "            # use \"with...as...\" to guarantee that the example is finished properly\n",
    "            with self.vw.example({'w': [word]}) as ex:\n",
    "                pred = self.sch.predict(examples=ex, my_tag=n+1, oracle=pos, condition=[(n,'p'), (n-1, 'q')])\n",
    "                output.append(pred)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to train models, extract entities, and evaluate test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(training_set, num_labels=3):\n",
    "    '''\n",
    "    Train NER models for each category (location, organisation, and person). \n",
    "    The number of labels is 3 using the BIO encoding scheme\n",
    "    '''\n",
    "    vw = pyvw.vw(search=num_labels, search_task='hook', ring_size=1024)\n",
    "    sequenceLabeler = vw.init_search_task(SequenceLabeler)    \n",
    "    sequenceLabeler.learn(training_set)\n",
    "    \n",
    "    return sequenceLabeler\n",
    "\n",
    "def extract_entities(sentence, label_list):\n",
    "    '''\n",
    "    Takes in a sentence and a sequence of labels (from the Vowpal Wabbit NER classifier) as input.\n",
    "    Identifies subsequences in sentence where the classifier believes named entities are present. \n",
    "    It returns identified entities as a list of strings.\n",
    "    '''\n",
    "    entities = []\n",
    "    for pos, label in enumerate(label_list):\n",
    "        if label == 1:\n",
    "            pos2 = pos+1\n",
    "            current_entity = sentence[pos]\n",
    "            while pos2 < len(label_list) and label_list[pos2] == 2:\n",
    "                current_entity = current_entity + ' ' + sentence[pos2]\n",
    "                pos2 += 1\n",
    "            entities.append(current_entity)\n",
    "    return entities\n",
    "\n",
    "def eval_sentences(sentences):\n",
    "    '''\n",
    "    Takes in a list of sentences.\n",
    "    Runs each model's predict() method and extracts predicted entities from the sentence.\n",
    "    Returns these identified entities\n",
    "    '''\n",
    "    models = {'locations': loc_model, 'organisations': org_model, 'people': per_model}\n",
    "    for sentence in sentences:\n",
    "        print sentence\n",
    "        for key, model in models.iteritems():\n",
    "            split_sentence = sentence.split()\n",
    "            vw_sentence = [(0, word) for word in split_sentence]\n",
    "            predicted_labels = model.predict(vw_sentence)\n",
    "            predicted_entities = extract_entities(sentence=split_sentence, label_list=predicted_labels)\n",
    "            print key, '-', predicted_entities\n",
    "        print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset contains only one type of entity - either locations, persons, or organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_train = joblib.load('conll_atis_merged_loc_train.pkl') #locations from CONLL 2003 and ATIS\n",
    "loc_model = train_model(loc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 'new'),\n",
      "  (2, 'york'),\n",
      "  (3, 'to'),\n",
      "  (1, 'las'),\n",
      "  (2, 'vegas'),\n",
      "  (3, 'on'),\n",
      "  (3, 'sunday'),\n",
      "  (3, 'afternoon')],\n",
      " [(1, 'new'),\n",
      "  (2, 'york'),\n",
      "  (3, 'to'),\n",
      "  (1, 'las'),\n",
      "  (2, 'vegas'),\n",
      "  (3, 'on'),\n",
      "  (3, 'sunday'),\n",
      "  (3, 'afternoon')],\n",
      " [(1, 'st'), (2, 'helens'), (3, ','), (1, 'england'), (3, '1996-08-26')],\n",
      " [(3, 'what'),\n",
      "  (3, 'flights'),\n",
      "  (3, 'from'),\n",
      "  (1, 'denver'),\n",
      "  (3, 'to'),\n",
      "  (1, 'salt'),\n",
      "  (2, 'lake'),\n",
      "  (2, 'city')]]\n"
     ]
    }
   ],
   "source": [
    "pprint([sentence for sentence in loc_train for label, word in sentence if label == 2 and len(sentence) <= 8][8:12])\n",
    "# {'B-LOC': 1, 'I-LOC': 2, 'Other': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "org_train = joblib.load('../datasets/conll_org_train.pkl') #Organisations from CONLL 2003\n",
    "org_model = train_model(org_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 'stockport'), (3, '0'), (1, 'notts'), (2, 'county'), (3, '0')],\n",
      " [(1, 'chernomorets'),\n",
      "  (2, 'novorossiisk'),\n",
      "  (3, '2'),\n",
      "  (1, 'rostselmash'),\n",
      "  (2, 'rostov'),\n",
      "  (3, '1')],\n",
      " [(1, 'chernomorets'),\n",
      "  (2, 'novorossiisk'),\n",
      "  (3, '2'),\n",
      "  (1, 'rostselmash'),\n",
      "  (2, 'rostov'),\n",
      "  (3, '1')],\n",
      " [(3, '--'),\n",
      "  (1, 'sydney'),\n",
      "  (2, 'newsroom'),\n",
      "  (3, '61-2'),\n",
      "  (3, '9373'),\n",
      "  (3, '1800')]]\n"
     ]
    }
   ],
   "source": [
    "pprint([sentence for sentence in org_train for label, word in sentence if label == 2 and len(sentence) <= 6][10:14])\n",
    "# {'B-ORG': 1, 'I-ORG': 2, 'Other': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_train = joblib.load('../datasets/conll_per_train.pkl') #Persons from CONLL 2003\n",
    "per_model = train_model(per_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(3, '1.'),\n",
      "  (1, 'tom'),\n",
      "  (2, 'pukstys'),\n",
      "  (3, '('),\n",
      "  (3, 'u.s.'),\n",
      "  (3, ')'),\n",
      "  (3, '86.82')],\n",
      " [(3, 'shelbourne'),\n",
      "  (3, '-'),\n",
      "  (1, 'mark'),\n",
      "  (2, 'rutherford'),\n",
      "  (3, '('),\n",
      "  (3, '5th'),\n",
      "  (3, ')')],\n",
      " [(1, 'fred'),\n",
      "  (2, 'trueman'),\n",
      "  (3, '('),\n",
      "  (3, 'england'),\n",
      "  (3, ')'),\n",
      "  (3, '307'),\n",
      "  (3, ','),\n",
      "  (3, '67')],\n",
      " [(3, '5.'),\n",
      "  (1, 'jamie'),\n",
      "  (2, 'baulch'),\n",
      "  (3, '('),\n",
      "  (3, 'britain'),\n",
      "  (3, ')'),\n",
      "  (3, '45.08')]]\n"
     ]
    }
   ],
   "source": [
    "pprint([sentence for sentence in per_train for label, word in sentence if label == 2 and len(sentence) <= 8][10:14])\n",
    "# {'B-PER': 1, 'I-PER': 2, 'Other': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new york to las vegas on sunday afternoon\n",
      "organisations - []\n",
      "locations - ['new york', 'las vegas']\n",
      "people - []\n",
      "\n",
      "chennai to mumbai on sunday afternoon\n",
      "organisations - []\n",
      "locations - []\n",
      "people - []\n",
      "\n",
      "lima to ascuncion on a sunday afternoon\n",
      "organisations - []\n",
      "locations - []\n",
      "people - []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_sentences = ['new york to las vegas on sunday afternoon',\n",
    "                    'chennai to mumbai on sunday afternoon',\n",
    "                    'lima to ascuncion on a sunday afternoon']\n",
    "eval_sentences(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st helens, england 1996-08-26\n",
      "organisations - ['st helens, england 1996-08-26']\n",
      "locations - ['england']\n",
      "people - []\n",
      "\n",
      "bishkek, kyrgyzstan 1996-08-26\n",
      "organisations - []\n",
      "locations - []\n",
      "people - []\n",
      "\n",
      "dar-es-salaam, tanzania 1996-08-26\n",
      "organisations - []\n",
      "locations - []\n",
      "people - []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_sentences2 = ['st helens, england 1996-08-26',\n",
    "                     'bishkek, kyrgyzstan 1996-08-26',\n",
    "                     'dar-es-salaam, tanzania 1996-08-26']\n",
    "eval_sentences(sample_sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill gates is the ceo of microsoft in seattle\n",
      "organisations - ['microsoft', 'seattle']\n",
      "locations - ['seattle']\n",
      "people - ['bill gates']\n",
      "\n",
      "jack ma is the ceo of alibaba in hangzhou\n",
      "organisations - []\n",
      "locations - []\n",
      "people - ['jack ma']\n",
      "\n",
      "narayana moorthy is the ceo of infosys in bengaluru\n",
      "organisations - []\n",
      "locations - []\n",
      "people - []\n",
      "\n",
      "larry page is the ceo of tesla in mountain view\n",
      "organisations - []\n",
      "locations - []\n",
      "people - ['larry page']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_sentences3 = ['bill gates is the ceo of microsoft in seattle',\n",
    "                     'jack ma is the ceo of alibaba in hangzhou',\n",
    "                     'narayana moorthy is the ceo of infosys in bengaluru',\n",
    "                     'larry page is the ceo of tesla in mountain view']\n",
    "eval_sentences(sample_sentences3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
